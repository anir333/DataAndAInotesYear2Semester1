{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exercises: Decision trees, random forests and K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. We load the iris dataset for you in the next code. We provide you with the X and y(=target) variable.\n",
    " a. Count the different values for y\n",
    " b. Fit a decision tree on this set. Use cross-validation and calculate the accuracy on the validation sets.\n",
    "    What other metrics are important for classification problems like this one?\n",
    " c. Makes it any difference if we use  a standard scaler?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-04T16:05:34.736601Z",
     "start_time": "2024-11-04T16:05:34.569753Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the wine dataset and take 'target' as the value you need to predict\n",
    "wine = load_wine()\n",
    "\n",
    "# Convert to a Pandas DataFrame for easier exploration\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Series(wine.target)\n",
    "X.head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-04T15:37:31.869181Z",
     "start_time": "2024-11-04T15:37:31.854496Z"
    }
   },
   "source": [
    "#SOLUTION_START\n",
    "print(y.value_counts()) #print the different values\n",
    "#SOLUTION_END"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    71\n",
      "0    59\n",
      "2    48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-04T15:55:01.477724Z",
     "start_time": "2024-11-04T15:55:01.379174Z"
    }
   },
   "source": [
    "#SOLUTION_START\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8, test_size=0.2, stratify=y)\n",
    "\n",
    "# Create a pipeline with scaling and Decision Tree\n",
    "pipeline = Pipeline([('standard_scaler',StandardScaler()),\n",
    "                     ('decision_tree', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "# model = DecisionTreeClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"\\n\\nScores:\", scores)\n",
    "# Fit the model to the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "prediction = pipeline.predict(X_test)\n",
    "\n",
    "#other important metrics\n",
    "print(f\"Test Accuracy: {pipeline.score(X_test, y_test):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, prediction))\n",
    "#other important metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, prediction))\n",
    "#SOLUTION_END"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scores: [0.91666667 0.80555556 0.83333333 0.91428571 0.85714286]\n",
      "Test Accuracy: 0.9444\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.96      0.94      0.95        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n",
      "Confusion Matrix:\n",
      " [[11  1  0]\n",
      " [ 0 14  0]\n",
      " [ 0  1  9]]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SOLUTION_START\n",
    "# Split the dataset into training and testing sets\n",
    "\n",
    "\n",
    "# Create a pipeline with scaling and Decision Tree\n",
    "\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model to the training set\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "\n",
    "\n",
    "#other important metrics\n",
    "\n",
    "#SOLUTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2. We load the same iris dataset for you in the next code. We provide you again  with the X and y(=target) variable.\n",
    " a.  Fit a random forest with 100 decision trees on this set. Use cross-validation and calculate the accuracy on the validation sets (.\n",
    "    What other metrics are important for classification problems like this one?\n",
    " b. Is the result better than for the decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-04T16:05:40.823522Z",
     "start_time": "2024-11-04T16:05:39.604020Z"
    }
   },
   "source": [
    "#SOLUTION_START\n",
    "# Import RandomForestClassifier\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Series(wine.target)\n",
    "X.head()\n",
    "\n",
    "# Create a pipeline with scaling and Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),    \n",
    "    ('random_forest', RandomForestClassifier(random_state=42)) \n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model to the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "pipeline.predict(X_test)\n",
    "#SOLUTION_END"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 1, 0, 0, 1, 1, 2, 1, 2, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 2, 1, 2, 0, 2, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3. We load the same iris dataset for you in the next code. We provide you again  with the X and y(=target) variable.\n",
    " a.  Fit a K-means with 3 values on this set. Use cross-validation and calculate the accuracy on the validation sets (.\n",
    "    What other metrics are important for classification problems like this one?\n",
    " b. Is the result better than for the decision tree? Why?\n",
    " c. Visualize the clusters (using PCA to reduce to 2 dimensions for plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-04T16:09:37.810134Z",
     "start_time": "2024-11-04T16:09:37.639470Z"
    }
   },
   "source": [
    "#SOLUTION_START\n",
    "#a) Import necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Convert to a Pandas DataFrame for easier exploration\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Series(wine.target)\n",
    "\n",
    "# Standardize the features (important for KMeans since it's distance-based)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Compare the K-Means clusters to the actual wine classes\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, clusters))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, clusters))\n",
    "\n",
    "\n",
    "#b) the K-measn clustering will construct clusters which are not related to the target variable. It will construct clusters which have nothing to do with the 3 wine categories"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 0  0 59]\n",
      " [65  3  3]\n",
      " [ 0 48  0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        59\n",
      "           1       0.06      0.04      0.05        71\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.02       178\n",
      "   macro avg       0.02      0.01      0.02       178\n",
      "weighted avg       0.02      0.02      0.02       178\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SOLUTION_START\n",
    "#3c) Visualize the clusters (using PCA to reduce to 2 dimensions for plotting)\n",
    "\n",
    "#SOLUTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4. Load the concrete_data dataset and split in a training and a  test set (20% test set). What is the result of a decissiontree regressor  on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:17:02.253757Z",
     "start_time": "2024-10-13T22:17:01.951399Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SOLUTION_START\n",
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/concrete_data.csv')\n",
    "# Display the first few rows to understand the structure of the dataset\n",
    "\n",
    "\n",
    "# Features (input) and target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "\n",
    "# Initialize and fit the DecisionTreeRegressor\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "\n",
    "\n",
    "# Visualize actual vs predicted\n",
    "\n",
    "#SOLUTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4. Load the concrete_data dataset and split in a training and a  test set (20% test set). What is the result of a random forestregressor  on the test set? Do we see an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T21:53:55.609027Z",
     "start_time": "2024-10-13T21:53:54.203920Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SOLUTION_START\n",
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "\n",
    "# Features (input) and target variable\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "\n",
    "# Initialize and fit the RandomForestRegressor\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "\n",
    "\n",
    "# Visualize actual vs predicted\n",
    "\n",
    "#SOLUTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4. Load the concrete_data dataset and use random forest again. Use gridsearch to determine the optimal number of trees in our forest.(test the folowing number of trees [10, 20,40, 50, 100,150, 200,250, 300, 350,450,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T21:58:48.892119Z",
     "start_time": "2024-10-13T21:58:30.977967Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SOLUTION_START\n",
    "# Import necessary libraries\n",
    "\n",
    "# Features (input) and target variable\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "\n",
    "# Define the RandomForestRegressor model\n",
    "\n",
    "# Define the grid of parameters to search (different numbers of trees)\n",
    "\n",
    "\n",
    "# Set up the GridSearchCV with cross-validation\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "\n",
    "# Get the best parameters and best score\n",
    "\n",
    "# Train the RandomForestRegressor using the optimal number of trees\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize actual vs predicted\n",
    "\n",
    "#SOLUTION_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:05:07.879011Z",
     "start_time": "2024-10-13T22:04:52.072103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SOLUTION_START\n",
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "# Features (input) and target variable\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "\n",
    "# Define the RandomForestRegressor model\n",
    "\n",
    "# Define the grid of parameters to search (different numbers of trees)\n",
    "\n",
    "\n",
    "# Set up the GridSearchCV with cross-validation\n",
    "\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "\n",
    "\n",
    "# Get the best parameters and best score\n",
    "\n",
    "\n",
    "\n",
    "# Train the RandomForestRegressor using the optimal number of trees\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize actual vs predicted\n",
    "\n",
    "\n",
    "#SOLUTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5. Go to the scikit-learn documentation and look for RandomizedSearchCV. Apply it for 10 different guesses between 10 and 500. What is the result for R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:15:15.333187Z",
     "start_time": "2024-10-13T22:15:01.345555Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SOLUTION_START\n",
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "\n",
    "# Features (input) and target variable\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "\n",
    "\n",
    "\n",
    "# Define the RandomForestRegressor model\n",
    "\n",
    "\n",
    "# Define the grid of parameters to search (different numbers of trees)\n",
    "\n",
    "\n",
    "\n",
    "# Set up the GridSearchCV with cross-validation\n",
    "\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "\n",
    "\n",
    "# Get the best parameters and best score\n",
    "\n",
    "\n",
    "\n",
    "# Train the RandomForestRegressor using the optimal number of trees\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize actual vs predicted\n",
    "\n",
    "\n",
    "#SOLUTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6. Finally apply a K-means clustering on the concrete dataset.\n",
    "   a.What is the influence of a standard scaler on the clustering?\n",
    "   b.What is the optimal number of clusters (elbow method, mean silhouette score)?\n",
    "   c.Show for the cement and water features how the clusters are scattered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T22:29:01.220342Z",
     "start_time": "2024-10-13T22:28:58.247065Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Features for clustering\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "\n",
    "\n",
    "# Determine the optimal number of clusters using the elbow method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the elbow method\n",
    "\n",
    "\n",
    "# Elbow method plot\n",
    "\n",
    "\n",
    "\n",
    "# Silhouette score plot\n",
    "\n",
    "\n",
    "#b probably 6 will be the optimal if we view the graphs\n",
    "# Choose the optimal number of clusters (you can choose based on the plots)\n",
    "\n",
    "\n",
    "# Fit the KMeans model with the optimal number of clusters\n",
    "\n",
    "\n",
    "\n",
    "# Assign cluster labels to the original DataFrame\n",
    "\n",
    "\n",
    "#c Visualize the clustering result (plotting two features:cement and water)\n",
    "\n",
    "\n",
    "\n",
    "# Display the DataFrame with cluster labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

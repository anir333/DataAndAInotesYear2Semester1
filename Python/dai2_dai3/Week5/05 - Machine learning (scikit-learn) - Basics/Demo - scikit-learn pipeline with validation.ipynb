{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DEMO MACHINE LEARNING PIPELINE WITH SCIKIT-LEARN\n",
    "# (WITH TRAIN - TEST SPLIT AND VALIDATION)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "myRandomState=0\n",
    "mySampleSize=10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DATA PREPARATION"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = None\n",
    "import seaborn as sns \n",
    "iris = sns.load_dataset('iris')\n",
    "y = iris['species'] # Target feature to predict\n",
    "X = iris.copy().drop('species', axis=1) # Predictors\n",
    "\n",
    "print(type(iris), iris.shape)\n",
    "print(type(X), X.shape)\n",
    "print(type(y), y.shape)\n",
    "\n",
    "display(X.head(5))\n",
    "display(y.head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Explore data\n",
    "display(X.sample(mySampleSize, random_state=myRandomState))\n",
    "display(y.sample(mySampleSize, random_state=myRandomState))\n",
    "# Mind that the indexes of the sample of y might be different of the indexes\n",
    "# of the sample of X because of the random selection.\n",
    "# When using random_state with the same state, you should get the same \n",
    "# indexes.\n",
    "display(X.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If you want to be sure to get the same indexes, e.g. when you do not use\n",
    "# random_state, extract the selected indexes from the sample of X and use\n",
    "# those indexes to slice y, but this requires you to store the sample of\n",
    "# X \n",
    "X_smpl = X.sample(mySampleSize)\n",
    "display(X_smpl)\n",
    "display(X_smpl.index)\n",
    "y_smpl = y[X_smpl.index]\n",
    "display(y_smpl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SPLIT LABELED DATA INTO TRAIN - TEST SAMPLE"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SPLIT LABELED DATA INTO TRAIN - TEST SAMPLE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data randomly into 80% training set and 20% test set\n",
    "X_tr, X_tst, y_tr, y_tst = train_test_split(X, y, random_state=0, train_size=0.8)\n",
    "# (use random_state to be sure that every time the same random sample is drawn)\n",
    "\n",
    "print(type(X_tr), X_tr.shape)\n",
    "print(type(X_tst), X_tst.shape)\n",
    "print(type(y_tr), y_tr.shape)\n",
    "print(type(y_tst), y_tst.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MODEL SELECTION AND HYPERPARAMETER SELECTION (MODEL SPECIFIC)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MODEL SELECTION AND HYPERPARAMETER SELECTION (MODEL SPECIFIC)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "print(model)\n",
    "# List all selected hyperparameters\n",
    "print(model.get_params(deep=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DERIVE MODEL (TRAIN MODEL/FIT MODEL)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DERIVE MODEL (TRAIN MODEL/FIT MODEL)\n",
    "\n",
    "model.fit(X_tr,y_tr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DISPLAY MODEL (MODEL SPECIFIC)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DISPLAY MODEL (MODEL SPECIFIC)\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### VALIDATE MODEL USING TEST DATA"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# VALIDATE MODEL USING TEST DATA\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict target feature for the test data\n",
    "y_tst_pred = pd.Series(model.predict(X_tst), name='y_tst_pred')\n",
    "\n",
    "# Calculate the difference between predicted and real values for the test data\n",
    "err = pd.Series(y_tst_pred.reset_index(drop=True)!=y_tst.reset_index(drop=True), name='err').astype(int)\n",
    "display(pd.concat([y_tst.reset_index(drop=True), y_tst_pred.reset_index(drop=True), err], axis=1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Confusion matrix\n",
    "# Display as text (console output)\n",
    "class_labels = sorted(list(pd.concat([y_tst,y_tst_pred], axis=0).unique()))\n",
    "# Alternative : model.classes_\n",
    "cm = confusion_matrix(y_true = y_tst, y_pred = y_tst_pred) \n",
    "print('Predicted label')\n",
    "print(class_labels)\n",
    "print(cm)\n",
    "# Display as heatmap (nicer output in Jupyter)\n",
    "disp = sns.heatmap(cm, square=True, annot=True, cbar=True, cmap='Greys', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "disp.xaxis.tick_top()                # Put x-axis tickers on top\n",
    "disp.xaxis.set_label_position('top') # Put x-axis label on top"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Metrics\n",
    "acc = accuracy_score(y_true=y_tst, y_pred=y_tst_pred)\n",
    "prec = precision_score(y_true=y_tst, y_pred=y_tst_pred, average='weighted')\n",
    "rec = recall_score(y_true=y_tst, y_pred=y_tst_pred, average='weighted')\n",
    "f1 = f1_score(y_true=y_tst, y_pred=y_tst_pred, average='weighted')\n",
    "# Mind this is a multiclass classification problem, so precision, recall and F1 \n",
    "# is calculated by class and averaged.\n",
    "print(f'ACC : {acc:.3f} - PREC : {prec:.3f} - REC : {rec:.3f} - F1 : {f1:.3f}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The easiest way to get results by class is to use precision_recall_fscore_support\n",
    "classes = sorted(list(pd.concat([y_tst,y_tst_pred], axis=0).unique()))\n",
    "# Display precision/recall/fscore/support table as text (consule output)\n",
    "display(precision_recall_fscore_support(y_true=y_tst, y_pred=y_tst_pred))\n",
    "# Display precision/recall/fscore/support as pandas dataframe (nicer outputin Jupyter)\n",
    "display(pd.DataFrame(precision_recall_fscore_support(y_true=y_tst, y_pred=y_tst_pred), index=['prec','rec','fscore','sup'], columns=classes))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Or use classification_report\n",
    "print(classification_report(y_true=y_tst, y_pred=y_tst_pred, target_names=class_labels))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# APPLY MODEL ON NEW DATA"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# APPLY MODEL ON NEW DATA\n",
    "\n",
    "X_pred = pd.DataFrame([[10,10,10,10],[5,5,5,5]], columns=(X.columns))\n",
    "y_pred = model.predict(X_pred)\n",
    "print(y_pred)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
